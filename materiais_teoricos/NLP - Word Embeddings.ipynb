{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6eC6iN1RsQ8"
      },
      "source": [
        "# Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEJhwy9-Yu7x"
      },
      "source": [
        "Métodos tradicionais de vetorização textual, como TF-IDF e BoW, caputuram unicamente característica sintáticas, como contagem e frequência relativa de palavras. Por isso, variáveis como o contexto de uma palavra, muito importante para o significado do texto, não são levados em conta.\n",
        "\n",
        "Word Embedding é a técnica de representar as palavras de um texto como vetores *n-dimensionais* baseando-se na semântica. A ideia principal é que as relações de significado entre as palavras do vocabulário sejam representadas pelas relações matemáticas entre os vetores.\n",
        "\n",
        "Dessa forma, se:\n",
        "\n",
        " * *Cão* é sinônimo de *Cachorrro*\n",
        " * $Vetor(\\text{Cão})\\approx Vetor(\\text{Cachorro})$\n",
        "\n",
        " * *Rei* está para *Homem* como *Rainha* está para *Mulher*\n",
        " * $Vetor(\\text{Rei}) - Vetor(\\text{Homem}) \\approx Vetor(\\text{Rainha}) - Vetor(\\text{Mulher})$ \n",
        "\n",
        "\\\\\n",
        "Usualmente, os embeddings são aplicados em palavras, entretanto sua lógica pode ser estendida para setenças, documentos, caracteres, etc.\n",
        "\n",
        "A seguir, veremos como funciona um dos pricipais métodos de word embedding, o Word-to-Vector (**Word2Vec**) e a importância do contexto no processo.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkvNyevCSZvC"
      },
      "source": [
        "## **Word2Vec model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft7PrIwkSlwi"
      },
      "source": [
        "Word2Vec é um poderoso modelo desenvolvido por Thomas Mikolov et al. na Google. Ele permite a construção de vetores representativos de uma palavra basedo no contexto em que ela está inserido.\n",
        "\n",
        "Essa tarefa é desempenhada por uma rede neural artificial em uma tarefa de self-supervised lerning. Essa tarefa pode assumir duas formas:\n",
        "\n",
        "1. Predizer uma palavra dado seu contexto - CBOW\n",
        "2. Predizer o contexto dada a palavra - Skip-gram\n",
        "\n",
        "O contexto de uma palavra é o grupo de palavras que aparecem ao seu redor em um dado intervalo. Dessa forma, o modelo entende que palavras com contextos semelhantes devem também ser semelhantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0RYwVLtWRWZ"
      },
      "source": [
        "### **Word2vec Pré treinado**\n",
        "\n",
        "Como já indicado anteriormente, uma palavra é definida pelo seu contexto. Portanto, por mais que a escrita de um termo não mude (sintaxe), seu significado pode se alterar a depender do conjunto de textos trabalhado (semântica). O idel para uma aplicação de NLP é que os significados locais sejam mantidos.\n",
        "\n",
        "Entretanto, não é incomum utilizarmos vetores pré-treinados em grandes corpus textuais. Além de evitar a necessidade de treino no nosso corpus local, esses vetores geralmente são derivados de um gigante conjunto de textos, por isso, representam basicamente a língua em si. Caso não hajam termos muito específicos ou relações semânticas especiais capturadas, podemos utilizar esses vetores sem nenhum problema.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F035FJ0dYvuQ"
      },
      "source": [
        "**Explorando vetores pré treinados usando a biblioteca gensim**\n",
        "\n",
        "Dando sequência, vamos testar alguns vetores pré-treinados com auxílio da biblioteca Gensim.\n",
        "\n",
        "Os vetores têm dimensão 100 e foram treinados no modelo CBOW.\n",
        "\n",
        "O conjunto de vetores utilizados pode ser encontrado no link http://nilc.icmc.usp.br/embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adSoW7HdahdQ",
        "outputId": "5de0c1e0-1eb3-4842-ba76-0ccf203ad710"
      },
      "source": [
        "# Download dos vetores\n",
        "!wget http://143.107.183.175:22980/download.php?file=embeddings/word2vec/cbow_s100.zip -O vetores\n",
        "\n",
        "!unzip vetores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-13 00:58:17--  http://143.107.183.175:22980/download.php?file=embeddings/word2vec/cbow_s100.zip\n",
            "Connecting to 143.107.183.175:22980... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 326003567 (311M) [application/octet-stream]\n",
            "Saving to: ‘vetores’\n",
            "\n",
            "vetores             100%[===================>] 310.90M  6.53MB/s    in 41s     \n",
            "\n",
            "2021-07-13 00:58:59 (7.54 MB/s) - ‘vetores’ saved [326003567/326003567]\n",
            "\n",
            "Archive:  vetores\n",
            "  inflating: cbow_s100.txt           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTF2indCY1qA"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOsShiYScWVn"
      },
      "source": [
        "Carregando os vetores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFRQ3L2jaTnx"
      },
      "source": [
        "modelo = KeyedVectors.load_word2vec_format(\"cbow_s100.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPyRroAEbw5n",
        "outputId": "25874ec4-2c29-46d0-971e-6cc419174d7a"
      },
      "source": [
        "print( \"O tamanho do vocabulário é\", len(modelo.vocab) )\n",
        "print( \"Os vetores representativos têm dimensão\", modelo.vector_size )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O tamanho do vocabulário é 929606\n",
            "Os vetores representativos têm dimensão 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY49TbfI0hLr"
      },
      "source": [
        "Agora, já podemos visualizar o vetor de uma palavra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0kC3PeG0lkx",
        "outputId": "d5dbc7a4-8589-4777-caf5-291ecbddf4b1"
      },
      "source": [
        "modelo['cachorro']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6.56940e-02, -5.24444e-01, -2.53404e-01, -3.83782e-01,\n",
              "        2.30628e-01, -2.19650e-02,  2.29181e-01, -3.44491e-01,\n",
              "       -2.31080e-02, -4.52210e-02, -7.84500e-02, -1.89070e-01,\n",
              "       -4.34008e-01, -2.31833e-01,  2.18070e-02,  1.17643e-01,\n",
              "       -4.92100e-02, -3.72790e-02,  5.19500e-03,  1.10290e-01,\n",
              "        2.08756e-01,  3.65709e-01, -3.66521e-01,  4.40905e-01,\n",
              "        7.56790e-02,  1.24670e-02,  5.31000e-04,  6.10890e-02,\n",
              "        1.02861e-01,  8.34240e-02,  1.46790e-02, -3.30467e-01,\n",
              "       -2.45400e-02,  2.13797e-01,  1.36240e-01,  2.05673e-01,\n",
              "        3.02481e-01, -2.30895e-01,  1.26977e-01,  8.69480e-02,\n",
              "       -1.83192e-01,  4.13077e-01,  1.16000e-01, -2.80642e-01,\n",
              "        2.94802e-01, -2.33011e-01, -2.27428e-01, -1.27036e-01,\n",
              "       -2.33886e-01, -3.33360e-01,  4.80820e-02, -4.70092e-01,\n",
              "        8.13960e-02, -2.07700e-01, -3.04254e-01,  5.41740e-02,\n",
              "        3.90831e-01,  1.71692e-01, -6.97040e-02, -5.30480e-02,\n",
              "        2.20763e-01,  2.48582e-01,  3.18993e-01, -2.98709e-01,\n",
              "        2.46220e-02,  1.37126e-01,  2.75683e-01,  5.55950e-02,\n",
              "        4.57170e-02, -3.01731e-01, -3.39329e-01, -7.88130e-02,\n",
              "       -9.32860e-02, -3.49435e-01, -1.81662e-01, -1.94415e-01,\n",
              "       -2.25571e-01, -1.79292e-01, -3.49447e-01, -2.33086e-01,\n",
              "       -2.50040e-02, -1.26200e-02,  2.03930e-01,  1.41959e-01,\n",
              "        3.18327e-01, -1.61190e-01, -9.05550e-02,  1.26558e-01,\n",
              "       -1.88893e-01, -3.25852e-01,  1.80220e-02,  2.96988e-01,\n",
              "       -7.07510e-02,  1.18515e-01,  1.05203e-01,  1.45130e-01,\n",
              "        1.53527e-01,  3.68000e-04,  3.49560e-01, -8.75190e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoJ_jE9OdAqH"
      },
      "source": [
        "A funcionalidade *most_similar* do gensim permite visualizar o top-n de palavras mais semelhantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKzqNX0nd1VV",
        "outputId": "ffe83490-aa2a-4ca8-c68e-cd567982c621"
      },
      "source": [
        "# Mais semelhantes à palavra cão\n",
        "modelo.most_similar('cão', topn=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cachorro', 0.8746297359466553),\n",
              " ('pássaro', 0.8384866118431091),\n",
              " ('gambá', 0.8313096761703491),\n",
              " ('ogro', 0.8301891088485718),\n",
              " ('cãozinho', 0.8299412727355957)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eADEuG1kDSbf",
        "outputId": "dee6eefc-9978-4247-c07d-412336d94bd4"
      },
      "source": [
        "# Mais semelhantes à palavra novela\n",
        "modelo.most_similar('novela', topn=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('trama', 0.799685537815094),\n",
              " ('minissérie', 0.767881453037262),\n",
              " ('telenovela', 0.7656346559524536),\n",
              " ('radionovela', 0.7263081073760986),\n",
              " ('microssérie', 0.7131772041320801)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EBjzTb6fm7H",
        "outputId": "53324e7a-0181-4111-aa7f-601b1547f0db"
      },
      "source": [
        "# Mais semelhantes às palavras homem e rainha e diferentes de rei\n",
        "# homem + rainha - rei\n",
        "modelo.most_similar(positive=['homem', 'rainha'],\n",
        "                    negative=['rei'], topn=1 )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('moça', 0.6089643836021423)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTpKUXVECDpn",
        "outputId": "852415d6-bc72-4aa3-fd5d-0fab35e20d8a"
      },
      "source": [
        "# ator - homem + mulher\n",
        "modelo.most_similar(positive=['ator', 'mulher'],\n",
        "                    negative=['homem'], topn=1 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('atriz', 0.7755578756332397)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YpyUVcC1w7y"
      },
      "source": [
        "### **A arquitetura Word2Vec**\n",
        "\n",
        "Nesta sessão, vamos tentar entender como são treinados modelos word2vec.\n",
        "Como mencionado anteriormente, podemos adotar duas metodologias:\n",
        "\n",
        "1. **Continuous Bag of Words (CBOW)** - Predizer uma palavra dado seu contexto\n",
        "2. **Skip-gram** - Predizer o contexto dada a palavra\n",
        "\n",
        "Vamos seguir com o Skip-gram, mas todo o raciocínio pode ser usado também para o CBOW.\n",
        "\n",
        "**O método Skip-gram**\n",
        "Primeiramente, vamos definir exatamente o que é um contexto e como ele se relaciona com a palavra.\n",
        "\n",
        "O contexto são as palavras que aparecem ao redor de uma palavra central em um intervalo. Esse intervalo é chamado de janela (window).\n",
        "Abaixo podemos destacado ver o contexto da palavra \"*dura*\" com uma janela de tamanho 2 (window_size).\n",
        "\n",
        "<center>\n",
        "\"Água <font color=red> mole pedra</font> <font color=green> dura </font> <font color=red>tanto bate</font> até que fura\"\n",
        "</center>\n",
        "\n",
        "Dessa forma, o mapeamento input-output do nosso modelo ficaria da seguinte forma.\n",
        "\n",
        "| Input - Palavra central | Output - Contexto |\n",
        "|---|---|\n",
        "| dura | mole |\n",
        "| dura | pedra |\n",
        "| dura | tanto |\n",
        "| dura | bate |\n",
        "\n",
        "O modelo Word2Vec deve ser capaz de prever as palavras que fazem parte do contexto de \"dura\". Esse processo vai ser feito para cada palavra de cada texto do nosso corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQAyQ48s8gTn"
      },
      "source": [
        "**Os componentes do Skip-gram**\n",
        "\n",
        "O treinamento dos vetores ocorre através de uma rede neural artificial específica, que é treinada para predizer o conteúdo do contexto de uma palavra.\n",
        "Vamos explorar os componentes dessa rede.\n",
        "\n",
        "1. Input Vector - A entrada dessa rede é um *one-hot vector* de tamanho |V| (tamanho do vocabulário). A posição do vetor que contém o número 1 corresponde à posição da palavra central no vocabulário.\n",
        "\n",
        "<center>\n",
        "<a href=\"https://ibb.co/48wxKdF\"><img src=\"https://i.ibb.co/XSSPCj2/Word-Embedding1-1.png\" alt=\"Word-Embedding1\" border=\"0\" width='280' height=250></a>\n",
        "</center>\n",
        "\n",
        "2. Embedding matrix - Essa matriz contém todos os embeddings de cada palavra do nosso vocabulário, por isso seu tamanho é |V|*N, onde N é o tamanho dos vetores incorporados.\n",
        "\n",
        "3. Context matrix - Matriz intermediária, serve para extrair o vetor contexto de uma palavra.\n",
        "\n",
        "4. Output vector - Um vetor de dimensão |V| que contém a probabilidade de cada palavra do vocabulário ser o contexto da palavra central.\n",
        "\n",
        "Abaixo podemos ver um resumo de como a rede neural ficaria estruturada. \n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.ibb.co/Mc47XqH/Word-Embedding2-1.png\" alt=\"Word-Embedding1\" border=\"0\" width=500 height=262>\n",
        "</center>\n",
        "\n",
        "Após o treinamento, estaremos interessados apenas na *Embedding Matrix*, que é quem contém os vetores incorporados em si.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRBG6B7A_Ncn"
      },
      "source": [
        "**Limitações computacionais do modelo discutido e possíveis soluções**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h73MreB4_WMH"
      },
      "source": [
        "A principal desvantagem do método acima é o seu custo computacional. Ele precisa atualizar todos os pesos da rede para cada par *palavra central - palavra contexto* do nosso corpus. A solução é utilizar de processos de subamostragem e subamostragem negativa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FangCPRABIpv"
      },
      "source": [
        "**Subamostragem**\n",
        "\n",
        "Algumas palavras, que aparecem muito frequentemente no corpus, acabam não trazendo tanta informação contextual. Por isso, os criadores do Word2Vec desenvolveram um método para subamostrar certas palavras. Essas palavras serão removidas do texto, não sendo utilizadas como palavra central ou contexto. A decisão de remover uma palavra ou não é baseada no seu sampling rate, calculado da seguinte forma:\n",
        "\n",
        "<center>\n",
        "$\\displaystyle\n",
        "P(w_i) = \\left( \\sqrt{\\frac{f(w_i)}{t}} + 1\\right)\\frac{t}{f(w_i)}$\n",
        "</center>\n",
        "\n",
        "Onde $t$ é um parâmetro de limiar customizável, geralmente entre $0.0001$ e $0.001$, e $f(w_i)$ a frequência documental normalizada da palavra $w_i$.\n",
        "\n",
        "$P(w_i)$ é a probabilidade de uma palavra ser mantida no conjunto de treino.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HBiBiweFPOP"
      },
      "source": [
        "**Subamostragem Negativa**\n",
        "\n",
        "O treinamento da rede neural envolve atualizar todos os pesos a cada exemplo. Entretanto, em nossa implementação, conforme o tamanho do vocabulário e a dimensão do vetor de embedding aumentam, a quantidade de pesos da nossa rede pode ficar incrivelmente alto. O que significa um grande custo computacional.\n",
        "\n",
        "A ideia da subamostragem negativa é atualizar somente uma pequena parcela dos pesos a cada iteração da rede. Isto é feito pela seleção aleatória de palavras fora do contexto da palavra central, chamadas de \"negativas\".\n",
        "\n",
        "Dessa forma, sortearemos uma quantidade fixa de palavras \"negativas\" (fora de contexto) para atualizar em cada iteração. Também atualizaremos os pesos para a palavra \"positiva\" (contexto).\n",
        "\n",
        "*Um bom número de amostras fica entre 1 e 20, diminuindo conforme o dataset aumenta.*\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.ibb.co/PTd8h2L/Word-Embedding3.png\" alt=\"Word-Embedding1\" border=0 width=600 height=295>\n",
        "</center>\n",
        "\n",
        "\n",
        "A seleção de uma palavra é baseada em sua frequência total no corpus. A probabilidade de seleção é calculada da seguinte forma:\n",
        "\n",
        "<center>\n",
        "$\\displaystyle\n",
        "P(w_i) = \\frac{freq(w_i)}{\\sum_{j=0}^{j=|V|} freq(w_j)}$\n",
        "</center>\n",
        "\n",
        "Onde $freq(w_i)$ é a quantidade de vezes que uma palavra ocorre no corpus. \n",
        "\n",
        "Os autores originais do método também propõem uma variação da equação, elevando as frequências a uma potência de 3/4. Essa varição aumenta a probabilidade para palavras menos frequêntes e diminui para as mais frequêntes.\n",
        "\n",
        "<center>\n",
        "$\\displaystyle\n",
        "P(w_i) = \\frac{freq(w_i)^{3/4}}{\\sum_{j=0}^{j=|V|} \\left( freq(w_j)^{3/4} \\right)}$\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vixjve38TAiT"
      },
      "source": [
        "Esses dois métodos auxiliam a diminuir drásticamente o custo computacional necessário para criar embedding vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1EsEFqPaAQY"
      },
      "source": [
        "### Treinando um modelo Word2Vec "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyeDI6MgaU4Y"
      },
      "source": [
        "Agora que já sabemos o funcionamento fundamental do modelo Word2Vec, vamos treina-lo com a biblioteca Gensim.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijJz7sNlcz33"
      },
      "source": [
        "Inicialmente, vamos importar o modelo e definir/entender alguns de **seus** parâmetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsLFrP4eayCO"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "modelo_w2v = Word2Vec(size=20,\n",
        "                      window=3,\n",
        "                      min_count=1,\n",
        "                      sg=1,\n",
        "                      negative=5\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1_2ScHrb-Q5"
      },
      "source": [
        "1. **Size** - Inteiro, representa o tamanho dos embedded vectors.\n",
        "2. **Window** - Inteiro, o tamanho do intervalo de palavras que compõe o contexto.\n",
        "3. **min_count** - Inteiro, quantidade de vezes mínima que uma palavra deve aparecer no corpus textual para ser incluída no vocabulário.\n",
        "4. **sg** - O valor sg=1 indica que será utilizado o método skip-gram, sg=0 indica o CBOW.\n",
        "5. **negative** - Inteiro, quantidade de amostras negativas sorteadas em cada iteração. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D45y67fda9_"
      },
      "source": [
        "Vamos simular alguns dados para o treinamento. O modelo do Gensim recebe textos tokenizados como input do treino. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5-dsxf9lQUk",
        "outputId": "f6ce1aea-2d28-48c5-facc-7e684b12c3a7"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentencas = [\"Água mole, pedra dura, tanto bate até que fura\",\n",
        "             \"De grão em grão a galinha enche o papo\",\n",
        "             \"A esperança é a última que morre\",\n",
        "             \"A fome é o melhor tempero\",\n",
        "             \"A mentira tem perna curta\",\n",
        "             \"A pressa é a inimiga da perfeição\",\n",
        "             \"Amigos, amigos. negócios à parte\",\n",
        "             \"Casa de ferreiro, espeto de pau\"]\n",
        "tokenizer = RegexpTokenizer('\\w+')\n",
        "\n",
        "tokens = [ tokenizer.tokenize(sentenca) for sentenca in sentencas ]\n",
        "tokens = [ [token.lower() for token in sentenca] for sentenca in tokens ]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXcreHX9np-u"
      },
      "source": [
        "O primeiro passo do treinamento do modelo é a consolidação do vocabulário."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiwKtNxqnQd8"
      },
      "source": [
        "modelo_w2v.build_vocab(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FAbE_XuoFqt"
      },
      "source": [
        "Podemos visualizar as palavras selecionadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Luu7RMwgoH_l",
        "outputId": "3b4e2cfd-9e5e-46c8-f716-4dd2fa4c6830"
      },
      "source": [
        "modelo_w2v.wv.vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6e2e8>,\n",
              " 'amigos': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbd1208>,\n",
              " 'até': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6e208>,\n",
              " 'bate': <gensim.models.keyedvectors.Vocab at 0x7f5f3bc0f780>,\n",
              " 'casa': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbee4e0>,\n",
              " 'curta': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbd11d0>,\n",
              " 'da': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbd1f60>,\n",
              " 'de': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6e470>,\n",
              " 'dura': <gensim.models.keyedvectors.Vocab at 0x7f5f3bb22a20>,\n",
              " 'em': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6eb38>,\n",
              " 'enche': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6ec88>,\n",
              " 'esperança': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6e898>,\n",
              " 'espeto': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbee208>,\n",
              " 'ferreiro': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbee550>,\n",
              " 'fome': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6e518>,\n",
              " 'fura': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6e6a0>,\n",
              " 'galinha': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6e668>,\n",
              " 'grão': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6ed30>,\n",
              " 'inimiga': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbd1908>,\n",
              " 'melhor': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbd16a0>,\n",
              " 'mentira': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbd15f8>,\n",
              " 'mole': <gensim.models.keyedvectors.Vocab at 0x7f5f3bb22128>,\n",
              " 'morre': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6ea58>,\n",
              " 'negócios': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbd1ac8>,\n",
              " 'o': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6e748>,\n",
              " 'papo': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6e438>,\n",
              " 'parte': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbd1400>,\n",
              " 'pau': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbee160>,\n",
              " 'pedra': <gensim.models.keyedvectors.Vocab at 0x7f5f3bb22ef0>,\n",
              " 'perfeição': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbd14a8>,\n",
              " 'perna': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbd1b38>,\n",
              " 'pressa': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbd1e48>,\n",
              " 'que': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6e940>,\n",
              " 'tanto': <gensim.models.keyedvectors.Vocab at 0x7f5f3bb22198>,\n",
              " 'tem': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbd1358>,\n",
              " 'tempero': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbd1160>,\n",
              " 'à': <gensim.models.keyedvectors.Vocab at 0x7f5f3bbd1eb8>,\n",
              " 'água': <gensim.models.keyedvectors.Vocab at 0x7f5f3bb22240>,\n",
              " 'é': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6e128>,\n",
              " 'última': <gensim.models.keyedvectors.Vocab at 0x7f5f3be6ec50>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTt6BtE2oD_F"
      },
      "source": [
        "Na sequência, basta chamar o método de treino."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22pvWwtin1Oc",
        "outputId": "350ae52d-3cb0-4a40-d1ac-6b14cba98335"
      },
      "source": [
        "modelo_w2v.train(tokens, \n",
        "                 epochs=30, \n",
        "                 total_examples=len(sentencas))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(348, 1620)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ0CYiQBo2h9"
      },
      "source": [
        "Como podemos ver abaixo, o modelo já foi capaz de aprender os vetores para cada palavra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0bYWsP3oeOU",
        "outputId": "4ecb5937-801f-4b06-c20d-90bfdd345f11"
      },
      "source": [
        "modelo_w2v.wv['água']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.01068152,  0.00904823,  0.02018374, -0.00692262, -0.01716428,\n",
              "       -0.00625954,  0.01748959, -0.00256764, -0.01139172,  0.00751887,\n",
              "        0.0088778 , -0.00887354, -0.02470621, -0.01553497, -0.01994365,\n",
              "        0.00910266,  0.01669788,  0.0003144 , -0.0192175 ,  0.0062617 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pfl3DKZpA8M",
        "outputId": "5f7c71ae-0da7-46a2-8e26-1d97d7d81852"
      },
      "source": [
        "modelo_w2v.wv['pedra']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.01708624,  0.0141287 ,  0.00167886,  0.00514836,  0.00682403,\n",
              "        0.00201663, -0.02057892,  0.00030189,  0.01019321, -0.01384514,\n",
              "        0.00586941,  0.00047907,  0.00255555, -0.00478247, -0.00378135,\n",
              "       -0.0082641 , -0.01951862, -0.00827733,  0.00408386,  0.02334811],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4GRA0_TqPZl"
      },
      "source": [
        "### Limitações do Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JkhJ4bxqUTc"
      },
      "source": [
        "O método Word2vec foi uma ideia revolucionária no campo de processamento de liguagem natural. Entretanto, ainda possui limitações.\n",
        "\n",
        "Uma dos principais problemas é tentar ajustar um significado estático para um termo sintático.\n",
        "Como sabemos, por mais que a escrita seja constante, o significado de uma palavra varia com o contexto (paronímia).\n",
        "\n",
        "Como por exemplo, o termo \"cão\", que pode significar \"cachorro\" ou \"diabo\", a depender do contexto usado. O modelo Word2vec é incapaz de diferir esse comportamento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn2jS6sGHwwt"
      },
      "source": [
        "### Word mover’s distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbHWbDjlIFVc"
      },
      "source": [
        "**Word mover’s distance (WMD)** é uma métrica de distância entre textos baseada nos embedded vectors.\n",
        "\n",
        "Ela é definida como a distância mínima que os vetores das palavras do primeiro texto precisam \"viajar\" para se tornarem os vetores do segundo texto.\n",
        "\n",
        "O algoritmo computa a distância euclidiana par a par entre as palavras das duas frases e seleciona o menor custo possível para transformar todas as palavras da primeira frase na segunda.\n",
        "\n",
        "A classe KeyedVectors do Gensim já possui essa métrica implementada, e podemos utilizar os vetores pré-treinados carregados anteriormente.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jNToKEyHxbd"
      },
      "source": [
        "frase1 = \"O bolo deve ser assado na temperatura certa\"\n",
        "frase2 = \"Corte os legumes para o cozimento\"\n",
        "frase3 = \"Aprendizado de máquina é um sub campo da inteligência artificial\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9iXhajeXycJ",
        "outputId": "05f065e5-bbb4-41fb-b52c-11f9ab697614"
      },
      "source": [
        "print(\"Distância entre as frases 1 e 2: \", round(modelo.wmdistance(frase1, frase2),2))\n",
        "print(\"Distância entre as frases 1 e 3: \", round(modelo.wmdistance(frase1, frase3),2))\n",
        "print(\"Distância entre as frases 2 e 3: \", round(modelo.wmdistance(frase2, frase3),2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distância entre as frases 1 e 2:  0.72\n",
            "Distância entre as frases 1 e 3:  1.06\n",
            "Distância entre as frases 2 e 3:  0.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv2D88qKYrKx"
      },
      "source": [
        "As distâncias calculadas são logicamente coerentes com o esperado. As frases 1 e 2, que falam sobre instruções de cozinha, são mais próximas entre si que entre a frase 3, que trata de um assunto diferente.\n",
        "\n",
        "Além disso, por mais que os textos não compartilhem nenhuma palavra entre si, com a semântica capturada pelos vetores do Word2Vec, somos capazes de mensurar uma distância entre as sentenças com sucesso.\n",
        "\n",
        "Esse comportamento não pode ser replicado por métodos de vetorização baseados na sintaxe, como OneHotEncoding e TF-IDF."
      ]
    }
  ]
}