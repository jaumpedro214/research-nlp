{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Construindo o vocabulário NLP, tokenização e normalização textual.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP+mgT0Yh2SONJ4z6RAn1TS"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6C3k0DuQNcfC"},"source":["# Construindo o vocabulário NLP"]},{"cell_type":"markdown","metadata":{"id":"XKkmAzxVNhrv"},"source":["Esse texto abordará o conceito de vocabulário e os conceitos necessários para sua construção, abordando os principais métodos da área de NLP para essa tarefa."]},{"cell_type":"markdown","metadata":{"id":"SBcrvDRlPSzg"},"source":["## Tokenização"]},{"cell_type":"markdown","metadata":{"id":"0D0MEjdSRNFv"},"source":["O primeiro passo para a construção de um vocabulário é a quebra do texto em partes menores, chamadas **tokens**. **Tokenização** pode ser entedido como o processo de dividir o texto em fatias menores, mas que ainda contenham significado. Tokens geralmente são palavras, mas podem englobar números, pontuações, símbolos e emoticons. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZP5zgFctRJ3P","executionInfo":{"status":"ok","timestamp":1609654855475,"user_tz":180,"elapsed":631,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"38eac9fb-3143-465f-9a27-225302cb36c2"},"source":["texto = \"O rato roeu a roupa do rei de roma\"\n","texto.split()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['O', 'rato', 'roeu', 'a', 'roupa', 'do', 'rei', 'de', 'roma']"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"AT0H_CsgSOT_"},"source":["### Problemas com a Tokenização"]},{"cell_type":"markdown","metadata":{"id":"dSQCL_c0S2C_"},"source":["O processo de tokenização mais básico considera que os termos separados por espaços contém significado independentemente. Entretanto, isto não é sempre o caso. Vejamos as frases abaixo:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MXj1RrrVSQkH","executionInfo":{"status":"ok","timestamp":1609654855754,"user_tz":180,"elapsed":897,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"60123e67-c482-44db-dc00-c80e57c7e676"},"source":["texto = \"A cidade do Rio de Janeiro é muito bonita\"\n","texto.split()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['A', 'cidade', 'do', 'Rio', 'de', 'Janeiro', 'é', 'muito', 'bonita']"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"8zIjvpHcTKSg"},"source":["Neste caso, *Rio de Janeiro* deveria ser um único token pois, no contexto, as palavras *Rio*, *de* e *Janeiro* não apresentam significado individual.\n","\n","Tokens podem ter tamanhos variados. O mais comum é o **unigrama**, que contém apenas uma palavra, mas podem haver **n-gramas** de qualquer tamanho."]},{"cell_type":"markdown","metadata":{"id":"D2XaOendZ2_f"},"source":["A ideia é criar tokenizers que possam lidar de forma adequada com o contexto do qual o corpus textual foi extraído."]},{"cell_type":"markdown","metadata":{"id":"gigBOVHUOHkP"},"source":["### Diferentes tipos de Tokenizadores"]},{"cell_type":"markdown","metadata":{"id":"PV815I_vPDlO"},"source":["**Tokenizers baseados em Expressões Regulares**"]},{"cell_type":"markdown","metadata":{"id":"PrC-UPBJQHD3"},"source":["Expressões regulares são conjuntos de caracteres que determinam um padrão textual específico. São a forma mais popular de busca de padrões em textos. \n","\n","Por exemplo, se estamos em busca de um CPF em uma frase, sabemos que ele obedece a forma XXX.XXX.XXX-XX onde cada X é necessáriamente um número entre 0 e 9. Essa forma padrão do CPF é dita uma expresão regular, e com ela somos capazes de identificar qualquer CPF dentro de um texto. O python nativamente já possui uma biblioteca para trabalhar com expressões regulares.\n","\n","Saiba mais no [link](https://www.w3schools.com/python/python_regex.asp). "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1cEN57thOIUH","executionInfo":{"status":"ok","timestamp":1609654855755,"user_tz":180,"elapsed":888,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"21a61820-90ac-47d7-ab97-2caf96333498"},"source":["# Exemplo de busca por CPF\n","import re\n","\n","expressao_regular = \"[0-9][0-9][0-9].[0-9][0-9][0-9].[0-9][0-9][0-9]-[0-9][0-9]\"\n","\n","frase1 = \"Meu CPF é 123.456.789-11\"\n","frase2 = \"O CPF 987.654.321-10 pertence a Fulano\"\n","\n","print( re.findall(expressao_regular, frase1) )\n","print( re.findall(expressao_regular, frase2) )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['123.456.789-11']\n","['987.654.321-10']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iSHoqWRfTE9P"},"source":["Com o uso de Regex (Regualar Expressions), somos capazes de determinar padrões específicos no texto que devem ser captuados pelo tokenizador.\n","\n","O NLTK provê uma funcionalidade **regular expressions-based\n","tokenizers** (RegexpTokenizer), que faz exatamente isso."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kbsp9ZnRTdpe","executionInfo":{"status":"ok","timestamp":1609654856640,"user_tz":180,"elapsed":1763,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"afa53763-789d-4c61-9e32-a53b6f0ffd52"},"source":["from nltk.tokenize import RegexpTokenizer\n","\n","# O símbolo | representa o conectivo lógico OU\n","expressao_regular = '[0-9]{3}.[0-9]{3}.[0-9]{3}-[0-9]{2}|Rio de Janeiro|São Paulo|\\w+|\\S+'\n","\n","frase1 = \"Fulano mora no Rio de Janeiro, seu CPF é 987.654.321-10\"\n","frase2 = \"Cicrano, de cpf 123.456.789-11, mora em São Paulo\"\n","\n","tokenizer = RegexpTokenizer(expressao_regular)\n","print(tokenizer.tokenize(frase1))\n","print(tokenizer.tokenize(frase2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Fulano', 'mora', 'no', 'Rio de Janeiro', ',', 'seu', 'CPF', 'é', '987.654.321-10']\n","['Cicrano', ',', 'de', 'cpf', '123.456.789-11', ',', 'mora', 'em', 'São Paulo']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kahI_VMtd34P"},"source":["**TweetTokenizer**"]},{"cell_type":"markdown","metadata":{"id":"KX0t9v8Ke4wW"},"source":["Um exemplo de tokenizer contextual é o TweetTokenizer, feito especificamente para lidar com a linguagem utilizada em Tweets."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fgKkn4gIghaN","executionInfo":{"status":"ok","timestamp":1609654856642,"user_tz":180,"elapsed":1755,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"d97a2016-ad82-4e71-b074-b6745ad144b8"},"source":["from nltk.tokenize import TweetTokenizer\n","\n","tweet1 = \"@fulano_de_tal Consegui comprar o meu carro própriooooooo!!! :) :-D #felicidade #carroproprio\"\n","tokenizer = TweetTokenizer()\n","tokenizer.tokenize(tweet1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['@fulano_de_tal',\n"," 'Consegui',\n"," 'comprar',\n"," 'o',\n"," 'meu',\n"," 'carro',\n"," 'própriooooooo',\n"," '!',\n"," '!',\n"," '!',\n"," ':)',\n"," ':-D',\n"," '#felicidade',\n"," '#carroproprio']"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"0VGi-2LkhGm2"},"source":["O tokenizer foi capaz de lidar com as hastags *#felicidade* e *#carroproprio* e os emoticons *:-D* e *:)*.\n","\n","Além disso, ele possui dois parâmetros opcionais. O *reduce_len* serve para reduzir o uso excessivo de caracteres ao fim de uma palavra e o *strip_handles*, para remover cabeçalhos relacionados ao twitter.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TsEX_dEzlUhG","executionInfo":{"status":"ok","timestamp":1609654856643,"user_tz":180,"elapsed":1745,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"18c5d44e-3472-4846-9ef3-8fbd21b5b768"},"source":["tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n","tokenizer.tokenize(tweet1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Consegui',\n"," 'comprar',\n"," 'o',\n"," 'meu',\n"," 'carro',\n"," 'própriooo',\n"," '!',\n"," '!',\n"," '!',\n"," ':)',\n"," ':-D',\n"," '#felicidade',\n"," '#carroproprio']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"AP2RWHYkTleB"},"source":["## Entendendo normalização de palavras"]},{"cell_type":"markdown","metadata":{"id":"XBnQ2N5pWomr"},"source":["Na maioria dos casos, não precisamos conter cada palavra única do texto em nosso vocabulário. Textos são um tipo de informação muito ruidosa, preparar e selecionar corretamente quais palavras aparecerão no vocabulário ajuda a aumentar a representatividade da informação textual. As palavras *sou*, *é* e *somos* podem ser reduzidas a sua forma infinitiva *ser* sem grande perda de significado. De forma análoga, desinências verbais também podem ser removidas para manutenção do número de termos.\n","\n","Palavras como *a*, *uma* e *o*, que ocorrem frequêntemente, geralmente não são decisivas e não carregam muita informação para tarefas de Machine Learning, podendo ser totalmente removidas. Esse processo se chama **remoção de stopwords**.\n","\n","Vale salientar que essa etapa do processamento depende bastante do contexto trabalhado. "]},{"cell_type":"markdown","metadata":{"id":"3DCRmp2U5a08"},"source":["### Stemming"]},{"cell_type":"markdown","metadata":{"id":"MB_6l31469rU"},"source":["Stemming (Stemização) é o processo de remover alguns caracteres de  palavras inflexionadas, na tentativa de reduzi-las ao seu **radical** (stem).\n","\n","Por exemplo, as palavras *sapato*, *sapateiro* e *sapataria* são formadas pelo o radical *sapat* e por um **afixo** responsável pela inflexão da palavra. Ao reduzir todos os termos a *sapat*, tentamos enxugar nosso vocabulário enquanto mantemos o significado fundamental de cada um deles.\n","\n","O **Snowball stemmer** é um algoritmo de stemização disponível na NLTK que suporta a língua portuguesa.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9kIwdbz65cZZ","executionInfo":{"status":"ok","timestamp":1609654856643,"user_tz":180,"elapsed":1736,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"20fa71b2-6107-427c-b99c-e23aee88e760"},"source":["from nltk.stem.snowball import SnowballStemmer\n","\n","print(SnowballStemmer.languages)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_wV_PGKidiGw"},"source":["Vamos aplicar o stemmer em alguns exemplos."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_CDPZWNUdMqR","executionInfo":{"status":"ok","timestamp":1609654856644,"user_tz":180,"elapsed":1728,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"6b3e8263-cb5d-49b9-e779-ced84ae0dbb1"},"source":["stemmer = SnowballStemmer(language='portuguese')\n","\n","palavras = [\"casa\", \"casinha\", \"casebre\", \n","            \"porta\", \"portaria\", \"portão\", \n","            \"amar\", \"amaria\", \"amarei\",\n","            \"corri\", \"correis\",\"correríamos\"]\n","\n","for palavra in palavras:\n","  print( \"{:12s} - {:s}\".format(palavra, stemmer.stem(palavra)) )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["casa         - cas\n","casinha      - casinh\n","casebre      - casebr\n","porta        - port\n","portaria     - port\n","portão       - portã\n","amar         - amar\n","amaria       - amar\n","amarei       - amar\n","corri        - corr\n","correis      - corr\n","correríamos  - corr\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NVwHtDc5fZEQ"},"source":["### Lemmatization"]},{"cell_type":"markdown","metadata":{"id":"xAVXtfz-fbiA"},"source":["Enquanto o processo de Stemming envolve somente a remoção de caracteres ao fim da palavra, o Lemmatization tenta reduzir uma palavra a sua forma base considerando o seu contexto. Isso ajuda especialmente a agrupar palavras semelhantes, que serão reduzidas a uma mesma forma fundamental.\n","\n","Palavras como *leiteira*, *leiteiro* e *leitoso*, serão reduzidas a sua forma base *leite*. Verbos conjugados serão trazidos a sua forma infinitiva.\n","\n","Algoritmos de Lemmatization levam em conta o contexto da palavra, sua Classe gramatical, referida normalmente como **Part-of-speech (POS) tag**, etc.\n","\n","A NLTK não possui nenhum lemmatizer que suporte o português.\n","\n","Prosseguiremos utilizando a biblioteca Stanza, baseado-nos no link abaixo.\n","\n","\n","https://lars76.github.io/2018/05/08/portuguese-lemmatizers.html#3"]},{"cell_type":"code","metadata":{"id":"MMdtQAPfgb_3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609654861041,"user_tz":180,"elapsed":6123,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"e016d81e-946e-4c7f-834d-373ed8819a51"},"source":["!pip install stanza"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting stanza\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/8b/3a9e7a8d8cb14ad6afffc3983b7a7322a3a24d94ebc978a70746fcffc085/stanza-1.1.1-py3-none-any.whl (227kB)\n","\r\u001b[K     |█▍                              | 10kB 16.2MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20kB 22.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 30kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 40kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 61kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 71kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 81kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 92kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 102kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 112kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 122kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 133kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 143kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 153kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 163kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 174kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 184kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 194kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 204kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 215kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 225kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235kB 7.0MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.7.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.19.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.12.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (51.0.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n","Installing collected packages: stanza\n","Successfully installed stanza-1.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xnjKUz_0jQZA","executionInfo":{"status":"ok","timestamp":1609654882693,"user_tz":180,"elapsed":27765,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"eac246db-e084-4954-b003-e0b49aeff4ae"},"source":["import stanza\n","\n","stanza.download('pt')\n","nlp = stanza.Pipeline('pt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 18.4MB/s]                    \n","2021-01-03 06:21:14 INFO: Downloading default packages for language: pt (Portuguese)...\n","Downloading http://nlp.stanford.edu/software/stanza/1.1.0/pt/default.zip: 100%|██████████| 227M/227M [00:10<00:00, 20.8MB/s]\n","2021-01-03 06:21:29 INFO: Finished downloading models and saved to /root/stanza_resources.\n","2021-01-03 06:21:29 INFO: Loading these models for language: pt (Portuguese):\n","=======================\n","| Processor | Package |\n","-----------------------\n","| tokenize  | bosque  |\n","| mwt       | bosque  |\n","| pos       | bosque  |\n","| lemma     | bosque  |\n","| depparse  | bosque  |\n","=======================\n","\n","2021-01-03 06:21:30 INFO: Use device: cpu\n","2021-01-03 06:21:30 INFO: Loading: tokenize\n","2021-01-03 06:21:30 INFO: Loading: mwt\n","2021-01-03 06:21:30 INFO: Loading: pos\n","2021-01-03 06:21:31 INFO: Loading: lemma\n","2021-01-03 06:21:31 INFO: Loading: depparse\n","2021-01-03 06:21:32 INFO: Done loading processors!\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"mJzyCZ6OgcXo"},"source":["texto = \"Éramos dois e contrários. Ela encobrindo com a palavra o que eu publicava pelo silêncio\"\n","texto_info = nlp(texto)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8O-smfv0ghiv","executionInfo":{"status":"ok","timestamp":1609654890429,"user_tz":180,"elapsed":698,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"9514cd22-0559-4fb3-8b8e-4c94cf59893f"},"source":["print(\"{:12s}   {:12s}   {:s}\\n\".format(\"Palavra\", \"POS tag\", \"Lemma\") )\n","for sentenca in texto_info.sentences:\n","  for palavra in sentenca.words:\n","    print(\"{:12s}   {:12s}   {:s}\".format(palavra.text, palavra.upos, palavra.lemma) )\n","  print(40*\"-\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Palavra        POS tag        Lemma\n","\n","Éramos         AUX            ir\n","dois           NUM            dois\n","e              CCONJ          e\n","contrários     ADJ            contrário\n",".              PUNCT          .\n","----------------------------------------\n","Ela            PRON           ela\n","encobrindo     VERB           encobrir\n","com            ADP            com\n","a              DET            o\n","palavra        NOUN           palavra\n","o              PRON           o\n","que            PRON           que\n","eu             PRON           eu\n","publicava      VERB           publicar\n","por            ADP            por\n","o              DET            o\n","silêncio       NOUN           silêncio\n","----------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aWddpLqzoqpv"},"source":["Como podemos ver, o lematizador tenta encontrar a forma base de cada palavra. \n","\n","Assim como o Stemmer, ele não é perfeito e comete alguns erros.\n","Por exemplo, o lemma de *Éramos* deveria ser *Ser*.\n","\n","Entretanto, quando lidando com o termo *pelo*, o lemmatizer consegue um sucesso que o stemmer seria incapaz, já que a palavra *pelo* é uma aglutinação do verbo *por* mais o artigo *o*, comportamento indetectável pelo stemmer.\n","\n","Outra informação que podemos ver é a classe gramatical detectada. As siglas seguem o padrão abaixo:\n","\n","* ADJ: adjetivo\n","* ADP: adposição (sempre preposições em portug)\n","* ADV: advérbio\n","* AUX: auxiliar\n","* CCONJ: conjunção coordenada\n","* DET: determinante\n","* INTJ: interjeição\n","* NOUN: substantivo\n","* NUM: numeral\n","* PART: partícula\n","* PRON: pronome\n","* PROPN: nome próprio\n","* PUNCT: pontuação\n","* SCONJ: conjunção subordinada\n","* SYM: símbolo\n","* VERB: verbo\n","* X: outro"]},{"cell_type":"markdown","metadata":{"id":"h4toSNq2uYA2"},"source":["### Remoção de Stopwords"]},{"cell_type":"markdown","metadata":{"id":"TwMJvaSwDdd-"},"source":["O que são stopwords?\n","\n","Stopwords são palavras frequentes em um corpus textual que não carregam muito significado na maioria dos contextos. Geralmente são palavras necessárias para gerar coesão gramatical e coerência das frases.\n","\n","Não existe um consenso universal de quais palavras são definitivamente stopwords, vai depender da aplicação. Algumas bibliotecas de Machine Learning e NLP possuem listas de stopwords disponíveis em alguns idiomas, mas elas devem ser modificadas caso necessário.\n","\n","Vamos dar uma olhada na lista do NLTK de stopwords em português."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zAbt8i-C5mro","executionInfo":{"status":"ok","timestamp":1609654912215,"user_tz":180,"elapsed":672,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"f3265a79-9f22-478b-e5e1-d3349f5d102a"},"source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"id":"ClC2BUpC5wxy","executionInfo":{"status":"ok","timestamp":1609654994555,"user_tz":180,"elapsed":690,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"474aed2e-b1ae-408e-9834-6b79ad816658"},"source":["portugues_stopwords = stopwords.words('portuguese')\n","\", \".join(portugues_stopwords)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'de, a, o, que, e, é, do, da, em, um, para, com, não, uma, os, no, se, na, por, mais, as, dos, como, mas, ao, ele, das, à, seu, sua, ou, quando, muito, nos, já, eu, também, só, pelo, pela, até, isso, ela, entre, depois, sem, mesmo, aos, seus, quem, nas, me, esse, eles, você, essa, num, nem, suas, meu, às, minha, numa, pelos, elas, qual, nós, lhe, deles, essas, esses, pelas, este, dele, tu, te, vocês, vos, lhes, meus, minhas, teu, tua, teus, tuas, nosso, nossa, nossos, nossas, dela, delas, esta, estes, estas, aquele, aquela, aqueles, aquelas, isto, aquilo, estou, está, estamos, estão, estive, esteve, estivemos, estiveram, estava, estávamos, estavam, estivera, estivéramos, esteja, estejamos, estejam, estivesse, estivéssemos, estivessem, estiver, estivermos, estiverem, hei, há, havemos, hão, houve, houvemos, houveram, houvera, houvéramos, haja, hajamos, hajam, houvesse, houvéssemos, houvessem, houver, houvermos, houverem, houverei, houverá, houveremos, houverão, houveria, houveríamos, houveriam, sou, somos, são, era, éramos, eram, fui, foi, fomos, foram, fora, fôramos, seja, sejamos, sejam, fosse, fôssemos, fossem, for, formos, forem, serei, será, seremos, serão, seria, seríamos, seriam, tenho, tem, temos, tém, tinha, tínhamos, tinham, tive, teve, tivemos, tiveram, tivera, tivéramos, tenha, tenhamos, tenham, tivesse, tivéssemos, tivessem, tiver, tivermos, tiverem, terei, terá, teremos, terão, teria, teríamos, teriam'"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"FqBHPgJq6yNy"},"source":["A lista acima contém uma coleção genérica do que o NLTK entende por Stopwords em português. Entretanto, como já discutido acima, em contextos específicos algumas dessas palavras podem ser necessárias.\n","\n","O procedimento de remoção de stopwords geralmente ocorre logo após a tokenização."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tmSxi1jg8ZOR","executionInfo":{"status":"ok","timestamp":1609656177979,"user_tz":180,"elapsed":644,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"d3966a30-faf5-4536-f7fa-0b01f2692c9d"},"source":["frase = \"Você já reparou nos olhos dela? São assim de cigana oblíqua e dissimulada\"\n","tokens = frase.split()\n","frase_sem_stopwords = [ token for token in tokens if token not in portugues_stopwords ]\n","frase_sem_stopwords"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Você',\n"," 'reparou',\n"," 'olhos',\n"," 'dela?',\n"," 'São',\n"," 'assim',\n"," 'cigana',\n"," 'oblíqua',\n"," 'dissimulada']"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"t_2BlPV1_fdB"},"source":["### Case folding"]},{"cell_type":"markdown","metadata":{"id":"V5Kpd-Gz_gaj"},"source":["Outra estratégia adotada para normalizar os textos é o **case folding**. Em linhas gerais, o procedimento vai deixar todas as letras em **lowercase**.\n","\n","Neste caso, as palavras *Casa* e *casa* serão resumidas somente ao termo *casa*, o que não trás muita diferença de significado.\n","\n","Entretanto, em casos onde a presença de nomes próprios e siglas for de maior importância, o case folding pode não ser uma boa opção. Por exemplo, o termo *Rio*, referente à cidade, perde totalmente o significado se for reduzido à forma *rio*.\n","\n","O ideal é que possamos detectar quais palavras podem ou não sofrer a alteração de caso.\n","\n","A maioria das bibliotecas de NLP já trabalha automaticamente com os termos em lowercase. Abaixo podemos ver uma implementação simples em python nativo. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"5CYLaph_BLHf","executionInfo":{"status":"ok","timestamp":1609656772595,"user_tz":180,"elapsed":631,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"ef7af451-1dc3-418f-85e2-bc35d3b6c884"},"source":["frase = \"Esta É Uma Frase\"\n","frase.lower()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'esta é uma frase'"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"gfV2gilFBfpB"},"source":["### N-Grams"]},{"cell_type":"markdown","metadata":{"id":"s5SjKe_5Bh2a"},"source":["Até o momento, todas as nossas análises se basearam em tokens de tamanho 1, que apenas comtém uma palavra. Entretanto, sentenças geralmente contém termos compostos, como *Rio de Janeiro*, *São Paulo*, *fim de semana* e *sala de estar*. \n","\n","Esses termos carregam significados inerentes de sua forma composta, e não podem ser separados. Por mais que termos compostos sejam relativamente incomuns, eles carregam uma boa quantidade de informação, e técnicas devem ser aplicadas para que eles possam ser detectados.\n","\n","No geral, esses termos são agrupados em n-grams. Um n-gram é um grupo de n palavras sequênciais em um texto. Dessa forma, quando n é 1, temos a tokenização normal. Quando n é 2, temos a tokenização com bigramas, que é um caso bem comum nas aplicações de NLP.\n","\n","A maioria dos casos se resume a trigramas ou menos. Em geral, algoritmos de NLP utilizam-se simultaneamente de unigramas, bigramas e trigramas.\n","\n","Os códigos abaixo mostram como gerar n-gramas com o NLTK. \n"]},{"cell_type":"code","metadata":{"id":"EWHmsZkLDGVY"},"source":["from nltk.util import ngrams\n","frase = \"O cantor Roberto Carlos nasceu no dia 19 de abril em Cachoeiro de Itapemirim\"\n","\n","tokens = frase.split()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hEFsYoB0Gn54","executionInfo":{"status":"ok","timestamp":1609658221438,"user_tz":180,"elapsed":605,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"7bc9b6ad-8912-41ea-e25a-2b56a3f81a40"},"source":["bigramas = list(ngrams(tokens, 2))\n","[\" \".join(token) for token in bigramas]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['O cantor',\n"," 'cantor Roberto',\n"," 'Roberto Carlos',\n"," 'Carlos nasceu',\n"," 'nasceu no',\n"," 'no dia',\n"," 'dia 19',\n"," '19 de',\n"," 'de abril',\n"," 'abril em',\n"," 'em Cachoeiro',\n"," 'Cachoeiro de',\n"," 'de Itapemirim']"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tmPb4Dy0Gqgo","executionInfo":{"status":"ok","timestamp":1609658221703,"user_tz":180,"elapsed":685,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"e9968c04-91da-4e84-c6a1-811896bfcebd"},"source":["trigramas = list(ngrams(tokens, 3))\n","[\" \".join(token) for token in trigramas]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['O cantor Roberto',\n"," 'cantor Roberto Carlos',\n"," 'Roberto Carlos nasceu',\n"," 'Carlos nasceu no',\n"," 'nasceu no dia',\n"," 'no dia 19',\n"," 'dia 19 de',\n"," '19 de abril',\n"," 'de abril em',\n"," 'abril em Cachoeiro',\n"," 'em Cachoeiro de',\n"," 'Cachoeiro de Itapemirim']"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"aHSiatjXG5KR"},"source":["Como podemos ver acima, os termos *Roberto Carlos* e *Cachoeiro de Itapemirim* só puderam ser verdadeiramente capturados pelo uso dos n-gramas."]},{"cell_type":"markdown","metadata":{"id":"_7H2oyeLHY0g"},"source":["## Resumo"]},{"cell_type":"markdown","metadata":{"id":"V-ugXbvW3N4Q"},"source":["Esse material abordou uma série de etapas decisivas na construção de um vocabulário. O pré processamento dos dados é uma etapa crucial nas aplicações de Machine Learning, e ganha uma contexto próprio quando aplicado ao processamento de sequências textuais.\n","\n","Os métodos ensinados acima são peças para que montemos uma pipiline de pré processamento, e podem ser mais ou menos necessários em cada caso de uso. O fato é, quando usados adequadamente, irão ajudar os passos subsequêntes do modelo de Machine Learning, consequêntemente gerando resultados melhores."]}]}