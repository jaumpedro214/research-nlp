{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformando textos em dados numéricos.ipynb","provenance":[],"authorship_tag":"ABX9TyNpqgU3OuWympf5GSmaUXAk"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WbtXYoxOxnLS"},"source":["# Transformando Texto em estruturas de dados"]},{"cell_type":"markdown","metadata":{"id":"UMDSrVQfyu2Q"},"source":["Dados textuais possuem uma dificuldade especial, já que não existe uma transformação direta entre eles e os números. Encontrar uma representação numérica para textos é um desafio.\n","\n","O mundo de NLP apresenta várias possíveis abordagens para tratarmos dos nossos textos, como vetores TF-IDF, vetores de contagem e Word2Vec. Para entedermos melhor cada um deles, precisamos entender dois conceitos fundamentais que mapeam todas as linguagens, a **sintaxe** e a **semântica**.\n","\n","Sintaxe define o conjunto de estruturas e regras que regem uma língua. É ela quem estuda e determina como as palavras devem ser dispostas nas frases.\n","\n","A Semântica, por sua vez, é a parte que analisa o significado dos termos em si. É na semântica que estão os aspectos de significado e interpretação do texto.\n","\n","Em geral, os métodos variam entre lidar com a sintaxe ou a semântica de um texto. A importância que se dá para cada uma delas depende fundamentalmente da aplicação, não havendo objetivamente uma melhor que a outra.\n","\n","Neste material, iremos focar inicialmente nos métodos sintáticos, que utilizam aspectos brutos da informação contida no texto, como a contagem de palavras, frequência dos termos no corpus textual, entre outros. "]},{"cell_type":"markdown","metadata":{"id":"MeAPdJGq7430"},"source":["## Arquitetura Bag-of-Words"]},{"cell_type":"markdown","metadata":{"id":"etr-cvTp8k58"},"source":["Uma primeira noção intuitiva de como representar textos numericamente é contar as palavras que existem nele. É nisso que se baseia o Bag-of-Words.\n","\n","Um pré requisito para utilizar essa abordagem é a pré definição do vocabulário da aplicação. \n","Cada texto será representado por um vetor unidimensional com tamanho igual ao vocabulário e cada entrada do vetor será a contagem de uma palavra.\n","\n","Vamos implementar isso utilizando o Scikit-learn e o NLTk, além de utilizar o pandas para visualização."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"TyEvVQFIAahr","executionInfo":{"status":"ok","timestamp":1610399185620,"user_tz":180,"elapsed":3821,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"47c8ab75-b7a7-48a1-a4c2-579c756cf4d3"},"source":["# Do NLTK, importamos as stopwords\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# Do sklean, importamos a classe CoutVectorizer, que faz exatamente o processo descrito acima\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import pandas as pd\n","\n","sentencas = [\"Passa, tempo, tic-tac.\",\n","             \"Tic-tac, passa, hora.\",\n","             \"Chega logo, Tic-tac.\"]\n","\n","vectorizer = CountVectorizer(stop_words=stopwords.words('portuguese'),\n","                             lowercase=True)\n","sentencas_vetorizadas = vectorizer.fit_transform(sentencas)\n","\n","vocabulario = list(vectorizer.vocabulary_.keys())\n","vocabulario.sort()\n","df = pd.DataFrame(sentencas_vetorizadas.todense(),\n","                  columns=vocabulario,\n","                  index=[f\"Frase {i}\" for i in range(len(sentencas))])\n","df"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chega</th>\n","      <th>hora</th>\n","      <th>logo</th>\n","      <th>passa</th>\n","      <th>tac</th>\n","      <th>tempo</th>\n","      <th>tic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Frase 0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>Frase 1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>Frase 2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         chega  hora  logo  passa  tac  tempo  tic\n","Frase 0      0     0     0      1    1      1    1\n","Frase 1      0     1     0      1    1      0    1\n","Frase 2      1     0     1      0    1      0    1"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"FWj5aoNh64ti"},"source":["list(vectorizer.vocabulary_.keys()).sort()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJzovI73UWcC"},"source":["Como podemos ver, o CountVectorizer gera a contagem de palavras do vocabulário em cada sentença. Com isso, nosso texto está representado de forma numérica e pode ser alimentada a um modelo de machine learning.\n","\n","Nesta seção, utilizamos o CountVectorizer do Sklearn com os parâmetros padrão. Entretanto, a classe é bem mais completa, com suporte para n-grams, corte de palavras do vocabulário por baixa ou alta frequência, número máximo de palavras no vocabulário, etc.\n","\n","Abaixo, podemos ver um exemplo de aplicação de n-grams."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"tnikCovWaAiq","executionInfo":{"status":"ok","timestamp":1610399185625,"user_tz":180,"elapsed":3784,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"78c1b6b9-f220-4ee8-f581-f553741a938d"},"source":["vectorizer = CountVectorizer(stop_words=stopwords.words('portuguese'),\n","                             lowercase=True,\n","                             ngram_range=(2,2))\n","\n","sentencas_vetorizadas = vectorizer.fit_transform(sentencas)\n","vocabulario = list(vectorizer.vocabulary_.keys())\n","vocabulario.sort()\n","df = pd.DataFrame(sentencas_vetorizadas.todense(),\n","                  columns=vocabulario,\n","                  index=[f\"Frase {i}\" for i in range(len(sentencas))])\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chega logo</th>\n","      <th>logo tic</th>\n","      <th>passa hora</th>\n","      <th>passa tempo</th>\n","      <th>tac passa</th>\n","      <th>tempo tic</th>\n","      <th>tic tac</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Frase 0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>Frase 1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>Frase 2</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         chega logo  logo tic  passa hora  ...  tac passa  tempo tic  tic tac\n","Frase 0           0         0           0  ...          0          1        1\n","Frase 1           0         0           1  ...          1          0        1\n","Frase 2           1         1           0  ...          0          0        1\n","\n","[3 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"Hf-LfdCwawUa"},"source":["**Limitações do BoW**\n","\n","O modelo de Bag-of-Words é uma abordagem simples para representar as sentenças textuais e possui algumas limitações. \n","\n","A contagem pode funcionar bem em casos onde o vocabulário é curto, porém escala mal para grandes corpos textuais. Além de gerar o clássico problema do *curse of dimensionality*, os vetores textuais são geralmente muito esparsos, o que pode dificultar o aprendizado de vários modelos de Machine Learning.\n","\n","Como sabemos, os textos são dados sequenciais e o significado de uma palavra depende muito da sua vizinhança, simplifica-los a somente um vetor de contagem trás uma grande perda de informação contextual.\n","\n","Por fim, mesmo que o BoW permita a seleção de termos baseado na frequência de documentos, esse mapeamento pode não ser tão preciso, e podemos acabar perdendo termos pouco frequentes mas dotados de grande significado.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BvB5JK5t_D9R"},"source":["## Vetores TF-IDF "]},{"cell_type":"markdown","metadata":{"id":"lAAObsSw_n9o"},"source":["A abordagem **TF-IDF** é a forma mais comum de vetorização ponderada de termos.\n","O método é similar à abordagem BoW explicada acima, transforma textos em vetores baseado basicamente na frequência dos termos do vocabulário. Entretanto, as entrados de cada termo agora são valores *ponderados*.\n","\n","O **TF-IDF** é calculado com base em dois coeficientes:  \n","\n","- [x] *Term Frequency* - **TF**: leva em consideração o quanto um termo ocorreu em um documento em relação ao tamanho do documento. A forma mais comum de cálculo do TF é simplesmente a frequência normalizada do termo:\n","\n","Dado uma palavra $w$ em um documento $d$\n","\n","$TF(w, d) = \\frac{\\text{Número de vezes que a palavra $w$ aparece no documento $d$}}{\\text{Quantidade total de palavras no documento}}$\n","\n","\n","- [x]  *Inverse Document Frequêncy* - **IDF**: Este coeficiente é responsável por dar significância aos termos baseados em sua frequência documental. O uso somente do TF nos daria pesos altos para palavras frequêntes, ou seja, palavras pouco frequêntes, mas dotadas de muita informação poderiam ser oprimidas.\n","A ideia por trás do uso IDF é mitigar isso aumentando o peso dos termos raros e diminuindo o peso dos frequêntes.\n","\n","O cálculo do IDF de uma palavra $w$ segue a seguinte fórmula:\n","\n","$IDF(w) = \\log\\left(\\frac{\\text{Número total de documentos}}{\\text{Número de documentos com a palavra $w$}}\\right)$\n","\n","Por fim, o peso de uma palavra $w$ em um documento $d$ será:\n","\n","$peso(w,d) = TF(w, d)\\times IDF(w)$\n","\n","---\n","\n","O Sklearn também possui um TFIDFVectorizer\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"KjtS9TqNf-q8","executionInfo":{"status":"ok","timestamp":1610399185631,"user_tz":180,"elapsed":3779,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"273137e0-7714-4a87-f5e9-c97321002825"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","sentencas = [\"Passa, tempo, tic-tac.\",\n","             \"Tic-tac, passa, hora.\",\n","             \"Chega logo, Tic-tac.\"]\n","\n","vectorizer = TfidfVectorizer(stop_words=stopwords.words('portuguese'),\n","                             lowercase=True)\n","sentencas_vetorizadas = vectorizer.fit_transform(sentencas)\n","\n","vocabulario = list(vectorizer.vocabulary_.keys())\n","vocabulario.sort()\n","df = pd.DataFrame(sentencas_vetorizadas.todense(),\n","                  columns=vocabulario,\n","                  index=[f\"Frase {i}\" for i in range(len(sentencas))])\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>chega</th>\n","      <th>hora</th>\n","      <th>logo</th>\n","      <th>passa</th>\n","      <th>tac</th>\n","      <th>tempo</th>\n","      <th>tic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Frase 0</th>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.504107</td>\n","      <td>0.391484</td>\n","      <td>0.66284</td>\n","      <td>0.391484</td>\n","    </tr>\n","    <tr>\n","      <th>Frase 1</th>\n","      <td>0.000000</td>\n","      <td>0.66284</td>\n","      <td>0.000000</td>\n","      <td>0.504107</td>\n","      <td>0.391484</td>\n","      <td>0.00000</td>\n","      <td>0.391484</td>\n","    </tr>\n","    <tr>\n","      <th>Frase 2</th>\n","      <td>0.608845</td>\n","      <td>0.00000</td>\n","      <td>0.608845</td>\n","      <td>0.000000</td>\n","      <td>0.359594</td>\n","      <td>0.00000</td>\n","      <td>0.359594</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            chega     hora      logo     passa       tac    tempo       tic\n","Frase 0  0.000000  0.00000  0.000000  0.504107  0.391484  0.66284  0.391484\n","Frase 1  0.000000  0.66284  0.000000  0.504107  0.391484  0.00000  0.391484\n","Frase 2  0.608845  0.00000  0.608845  0.000000  0.359594  0.00000  0.359594"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"HeIZY7EJgzDd"},"source":["**Limitações do TF-IDF**\n","\n","Embora os coeficientes calculados pelo TF-IDF ajudem a minimizar a perda de informação causada pelo CountVectorizer, ainda se baseiam puramente em aspectos sintáticos. \n","\n","Além disso, continua não caputrando as relações posicionais entre os termos e sendo bastante sensível do tamanho do vocabulário;"]},{"cell_type":"markdown","metadata":{"id":"5cZeEsecj2qd"},"source":["## Distância e similaridade entre vetores textuais"]},{"cell_type":"markdown","metadata":{"id":"Lo0kNLcNkoA-"},"source":["Esta seção se dedica a discutir como medir a similaridade entre vetores numéricos derivados dos dados textuais."]},{"cell_type":"markdown","metadata":{"id":"-l20pTY_mpY1"},"source":["**Similaridade do cosseno**"]},{"cell_type":"markdown","metadata":{"id":"qjY7SJWkm6e0"},"source":["Podemos considerar dois vetores similares se eles forem parecidos em direção e magnitude.\n","\n","A similaridade por cosseno é capaz de condensar essas duas características em apenas um número; o cosseno do ângulo entre eles. O valor dessa métrica varia entre na amplitude [-1, 1], sendo +1, a semelhança perfeita e -1, a oposição perfeita.\n","\n","$\\cos(\\theta) = \\frac{A\\cdot B}{||A||\\ ||B||}$\n","\n","Dessa forma, podemos calcular a similaridade entre dois textos como a similaridade por cosseno de seus vetores.\n","\n","O sklearn já possui o método implementado.\n","Abaixo, vemos o código que calcula a similaridade entre vetores TF-IDF."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BIYdvBmZpE9w","executionInfo":{"status":"ok","timestamp":1610399185632,"user_tz":180,"elapsed":3767,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"0129ea76-65bf-44c3-b19c-48152bfa7955"},"source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","sentencas = [\"Inteligência artificial é uma grande área do conhecimento\",\n","             \"Eu gosto da área de Inteligência artificial\",\n","             \"O Bicho preguiça é um grande mamífero\"]\n","\n","vectorizer = TfidfVectorizer(stop_words=stopwords.words('portuguese'),\n","                             lowercase=True)\n","sentencas_vetorizadas = vectorizer.fit_transform(sentencas)\n","\n","# Cálculo da matriz de similaridades cosseno\n","similaridades = cosine_similarity(sentencas_vetorizadas)\n","\n","print(f\"A similaridade cosseno do texto 1 com o 2 é {similaridades[0][1].round(2)}\")\n","print(f\"A similaridade cosseno do texto 2 com o 3 é {similaridades[1][2].round(2)}\")\n","print(f\"A similaridade cosseno do texto 1 com o 3 é {similaridades[0][2].round(2)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["A similaridade cosseno do texto 1 com o 2 é 0.58\n","A similaridade cosseno do texto 2 com o 3 é 0.0\n","A similaridade cosseno do texto 1 com o 3 é 0.17\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6Qxu2LGazLLM"},"source":["## Extra - Construindo um Chatbot"]},{"cell_type":"markdown","metadata":{"id":"dg4O2zCp0IkE"},"source":["Uma aplicação interessante dos conceitos ensinados acima é a criação de um chatbot simples. Como sabemos calcular a similaridade entre dois textos, podemos comparar uma pergunta feita por um usuário com várias perguntas do nosso banco de dados e responder uma resposta pré cadastrada.\n","\n","Primeiramente, vamos escrever algumas perguntas e suas respostas."]},{"cell_type":"code","metadata":{"id":"eTqtOmIJzOnr"},"source":["import numpy as np\n","\n","perguntas = [\n","  \"Oi, como vai?\",\n","  \"Como vai você?\",\n","  \"Como anda você?\",\n","  \"Qual seu nome?\",\n","  \"Me diga seu nome.\",\n","  \"Como você se chama?\",\n","  \"Qual seu animal favorito?\",\n","  \"Qual seu bicho preferido?\",\n","  \"De que animal você gosta?\",\n","  \"De que estilo de música você gosta?\",\n","  \"Qual seu cantor favorito?\",\n","  \"Qual seu tipo de música preferido?\",\n","]\n","\n","respostas_id = [\n","  0,0,0,\n","  1,1,1,\n","  2,2,2,\n","  3,3,3,\n","]\n","\n","respostas = [\"Vou bem, obrigado!\",\n","             \"Meu nome é Bot Fulano De Tal\",\n","             \"Meu animal favorito é a raposa\",\n","             \"Gosto bastante de Roberto Carlos\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iq6P2zyW1DYU"},"source":["Agora, vamos vetorizar as perguntas com TF-IDF."]},{"cell_type":"code","metadata":{"id":"KvZNkXuG1I4K"},"source":["vectorizer = TfidfVectorizer(lowercase=True)\n","perguntas_vec = vectorizer.fit_transform(perguntas)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RseSBW013RQV"},"source":["Para responder às perguntas do usuário, vamos analisar com qual pergunta do nosso banco de dados ela mais se parece."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OLKtfq_W2GB0","executionInfo":{"status":"ok","timestamp":1610399292370,"user_tz":180,"elapsed":19008,"user":{"displayName":"joão pedro da Silva Lima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7COMjyR2mCL8sPIplB1vmbwj-2m3NaAa7AitNxg=s64","userId":"05152286323404631212"}},"outputId":"02057f2a-e8a0-43a1-ae78-3952380aeecf"},"source":["pergunta_usuario = [input(\"Digite sua pergunta \\n\")]\n","pergunta_usuario_vec = vectorizer.transform(pergunta_usuario)\n","\n","# Cálculo da similaridade da pergunta do usuário com as do banco de dados\n","similaridades = cosine_similarity(pergunta_usuario_vec, perguntas_vec)[0]\n","\n","id = np.argmax(similaridades)\n","print(\"\")\n","print( \"ChatBot: \",respostas[ respostas_id[id] ] )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Digite sua pergunta \n","Que animal você mais gosta?\n","\n","ChatBot:  Meu animal favorito é a raposa\n"],"name":"stdout"}]}]}